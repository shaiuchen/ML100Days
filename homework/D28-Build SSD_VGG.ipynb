{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd import build_ssd\n",
    "from layers.box_utils import *\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torchvision\n",
    "import pickle\n",
    "from layers import box_utils\n",
    "from layers import Detect\n",
    "from layers import functions\n",
    "from layers import modules\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "\n",
    "from torch.autograd import Function\n",
    "from layers.box_utils import decode, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights into state dict...\n",
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\M1\\GitHub\\DL-CVMarathon\\homework\\Day031_ObjectDetection\\ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n"
     ]
    }
   ],
   "source": [
    "## 詳細模型結構可以參考ssd.py\n",
    "ssd_net=build_ssd('train', size=300, num_classes=21)\n",
    "ssd_net.load_weights('./demo/ssd300_mAP_77.43_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默認Config檔案在data/config.py內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_classes': 21,\n",
       " 'lr_steps': (80000, 100000, 120000),\n",
       " 'max_iter': 120000,\n",
       " 'feature_maps': [38, 19, 10, 5, 3, 1],\n",
       " 'min_dim': 300,\n",
       " 'steps': [8, 16, 32, 64, 100, 300],\n",
       " 'min_sizes': [30, 60, 111, 162, 213, 264],\n",
       " 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
       " 'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
       " 'variance': [0.1, 0.2],\n",
       " 'clip': True,\n",
       " 'name': 'VOC'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_net.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'num_classes': 21,\n",
    "    'lr_steps': (80000, 100000, 120000),\n",
    "    'max_iter': 120000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'VOC',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'aspect_ratios' : 使用六張Feature Map，每一張上方有預設的anchor boxes，Boxes aspect ratio可以自己設定\n",
    "### 'feature_maps' : 使用feature map大小為[38x38, 19x19, 10x10, 5x5, 3x3, 1x1]\n",
    "### 'min_sizes'、'max_sizes'可藉由下方算式算出，由作者自行設計\n",
    "### 'steps' : Feature map回放回原本300*300的比例，如38要回放為300大概就是8倍\n",
    "### 'variance' : Training 的一個trick，加速收斂，詳見：https://github.com/rykov8/ssd_keras/issues/53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'min_sizes'、'max_sizes' 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_sizes:  [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]\n",
      "max_sizes:  [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "## source:https://blog.csdn.net/gbyy42299/article/details/81235891\n",
    "min_dim = 300   ## 维度\n",
    "# conv4_3 ==> 38 x 38\n",
    "# fc7 ==> 19 x 19\n",
    "# conv6_2 ==> 10 x 10\n",
    "# conv7_2 ==> 5 x 5\n",
    "# conv8_2 ==> 3 x 3\n",
    "# conv9_2 ==> 1 x 1\n",
    "mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2'] ## prior_box來源層，可以更改。很多改進都是基於此處的調整。\n",
    "# in percent %\n",
    "min_ratio = 20 ## 這裡即是論文中所說的Smin的= 0.2，Smax的= 0.9的初始值，經過下面的運算即可得到min_sizes，max_sizes。\n",
    "max_ratio = 90\n",
    "step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))## 取一個間距步長，即在下面用於循環給比取值時起一個間距作用。可以用一個具體的數值代替，這裡等於17。\n",
    "min_sizes = []  ## 經過以下運算得到min_sizes和max_sizes。\n",
    "max_sizes = []\n",
    "for ratio in range(min_ratio, max_ratio + 1, step):\n",
    "    ## 從min_ratio至max_ratio + 1每隔步驟= 17取一個值賦值給比。注意範圍函數的作用。\n",
    "    ## min_sizes.append（）函數即把括號內部每次得到的值依次給了min_sizes。\n",
    "    min_sizes.append(min_dim * ratio / 100.)\n",
    "    max_sizes.append(min_dim * (ratio + step) / 100.)\n",
    "min_sizes = [min_dim * 10 / 100.] + min_sizes\n",
    "max_sizes = [min_dim * 20 / 100.] + max_sizes\n",
    "\n",
    "## steps: 這一步要仔細理解，即計算卷積層產生的prior_box距離原圖的步長，先驗框中心點的坐標會乘以step，\n",
    "## 相當於從特徵映射位置映射回原圖位置，比如conv4_3輸出特徵圖大小為38 *38，而輸入的圖片為300* 300，\n",
    "## 所以38 *8約等於300，所以映射步長為8.這是針對300* 300的訓練圖片。\n",
    "steps = [8, 16, 32, 64, 100, 300]  \n",
    "aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    " \n",
    "print('min_sizes: ',min_sizes)\n",
    "print('max_sizes: ',max_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default anchor boxes設計原理，看懂收穫很多\n",
    "##### 可以理解 SSD原文中 8732個anchors是怎麼來的\n",
    "##### 38×38×4+19×19×6+10×10×6+5×5×6+3×3×4+1×1×4=8732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        self.version = cfg['name']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        '''依照Feature map大小找出所有的pixel 中心'''\n",
    "        '''下方這兩個loop會找出W個x軸pixel對上W個y軸pixel，假如現在是在38x38的feature map上，就會有38x38個值'''\n",
    "        '''ex. [0,1],[0,2]..[0,37] [1,1],[1,2]..[1,37]..........[37,37]'''\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k] ## 如self.steps==8，就是先將原圖size normalize(/300)後再乘上8\n",
    "                # unit center x,y\n",
    "                '''中心點'''\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: min_size\n",
    "                '''/self.image_size 就是在做normalization '''\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                '''小的正方形box'''\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                '''大的正方形box'''\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    '''aspect ratio 2,3'''\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    '''aspect ratio 1/2,1/3'''\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PriorBox_Demo=PriorBox(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8732, 4])\n"
     ]
    }
   ],
   "source": [
    "print(PriorBox_Demo.forward().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 如何設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        '''有幾類'''\n",
    "        self.num_classes = num_classes\n",
    "        '''判定為正樣本的threshold，一般設為0.5'''\n",
    "        self.threshold = overlap_thresh\n",
    "        '''background自己會有一類，不用Label，假如我們有20類一樣標註0-19，下方會自己空出一類給background'''\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        '''OHEM，找出分得最不好的樣品，也就是confidence score比較低的正負樣品'''\n",
    "        self.do_neg_mining = neg_mining\n",
    "        '''負樣品與正樣品的比例，通常是3:1'''\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = cfg['variance']\n",
    "     \n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "\n",
    "        '''prediction會output三個值'''\n",
    "        '''loc shape: bounding box 資訊，torch.size(batch_size,num_priors,4)'''\n",
    "        '''conf shape: 每一個bounding box 的信心程度，torch.size(batch_size,num_priors,num_classes)'''\n",
    "        '''priors shape: 預設的defaul box， torch.size(num_priors,4)'''\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            '''jaccard 計算每一個BBOX與ground truth的IOU'''\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "        '''用Variable包裝'''\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        '''smooth_l1_loss 計算bounding box regression'''\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        loss_c[pos] = 0\n",
    "        '''排列confidence 的分數'''\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        '''負樣品取出數量 == negpos_ratio*num_pos'''\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        '''用cross_entropy做分類'''\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        #double轉成torch.float64\n",
    "        N = num_pos.data.sum().double()\n",
    "        loss_l = loss_l.double()\n",
    "        loss_c = loss_c.double()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生我們Loss function，注意這裡的class要包含背景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use_cuda=False\n",
    "criterion = MultiBoxLoss(21, 0.5, True, 0, False, 3, 0.5,False, Use_cuda,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights into state dict...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "ssd_net=build_ssd('train', size=300, num_classes=21)\n",
    "use_pretrained=True\n",
    "if use_pretrained:\n",
    "    ssd_net.load_weights('./demo/ssd300_mAP_77.43_v2.pth')\n",
    "net=ssd_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''要不要使用gpu'''\n",
    "Use_cuda=False\n",
    "\n",
    "'''tensor type會依照cpu或gpu有所不同'''\n",
    "if torch.cuda.is_available():\n",
    "    if args.cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
    "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "'''使用GPU時可以開啟DataParallel，但當Input是不定大小時，要關掉'''\n",
    "if Use_cuda:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "'''使用GPU時模型要轉成cuda'''\n",
    "if Use_cuda:\n",
    "    net = net.cuda()\n",
    "    \n",
    "batch_size_=1\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.00001/batch_size_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 這裡我們先示範輸入的 image,Label格式，真正在訓練時，準備成一樣格式即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LJY\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "'''輸入影像格式，假設batch size 為 4'''\n",
    "image_in=torch.tensor(torch.rand(4,3,300,300),dtype=torch.float32)\n",
    "'''Label格式，沒有固定長度，看圖像中有幾個label就有幾個'''\n",
    "label_0=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
    "label_1=[[ 0.1804,  0.6076,  0.7701,  0.8485, 13.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 11.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 7.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 5.0000],]\n",
    "label_2=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 14.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
    "label_3=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=300\n",
    "iteration=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LJY\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "C:\\Users\\LJY\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, iteration: 10\n",
      "BBOX Regression Loss:  0.8114645851982963\n",
      "Classification Loss:  2.3429956506799767\n",
      "Epoch: 0, iteration: 20\n",
      "BBOX Regression Loss:  0.7001059832396331\n",
      "Classification Loss:  2.1407034485428422\n",
      "Epoch: 0, iteration: 30\n",
      "BBOX Regression Loss:  0.60612074887311\n",
      "Classification Loss:  1.944851524447217\n",
      "Epoch: 0, iteration: 40\n",
      "BBOX Regression Loss:  0.5225039707289801\n",
      "Classification Loss:  1.7638770262400307\n",
      "Epoch: 0, iteration: 50\n",
      "BBOX Regression Loss:  0.44930041260189474\n",
      "Classification Loss:  1.5983538634688768\n",
      "Epoch: 0, iteration: 60\n",
      "BBOX Regression Loss:  0.39225010989624776\n",
      "Classification Loss:  1.4392987427888095\n",
      "Epoch: 0, iteration: 70\n",
      "BBOX Regression Loss:  0.34805853682220295\n",
      "Classification Loss:  1.2948674444168335\n",
      "Epoch: 0, iteration: 80\n",
      "BBOX Regression Loss:  0.31203171334884783\n",
      "Classification Loss:  1.167592030984384\n",
      "Epoch: 0, iteration: 90\n",
      "BBOX Regression Loss:  0.28171760771009663\n",
      "Classification Loss:  1.05757773432712\n",
      "Epoch: 0, iteration: 100\n",
      "BBOX Regression Loss:  0.2559716462868231\n",
      "Classification Loss:  0.9630568035443623\n",
      "Epoch: 0, iteration: 110\n",
      "BBOX Regression Loss:  0.2340310096038311\n",
      "Classification Loss:  0.8819680552289944\n",
      "Epoch: 0, iteration: 120\n",
      "BBOX Regression Loss:  0.21523548555962832\n",
      "Classification Loss:  0.8122287388385079\n",
      "Epoch: 0, iteration: 130\n",
      "BBOX Regression Loss:  0.19903535335364503\n",
      "Classification Loss:  0.7519847546038125\n",
      "Epoch: 0, iteration: 140\n",
      "BBOX Regression Loss:  0.18516060062856585\n",
      "Classification Loss:  0.6996744654244847\n",
      "Epoch: 0, iteration: 150\n",
      "BBOX Regression Loss:  0.17300970151100628\n",
      "Classification Loss:  0.6539743299469536\n",
      "Epoch: 0, iteration: 160\n",
      "BBOX Regression Loss:  0.1623104468996947\n",
      "Classification Loss:  0.6137655795517343\n",
      "Epoch: 0, iteration: 170\n",
      "BBOX Regression Loss:  0.1528898314104166\n",
      "Classification Loss:  0.5781490509611329\n",
      "Epoch: 0, iteration: 180\n",
      "BBOX Regression Loss:  0.14448732012123974\n",
      "Classification Loss:  0.546402643958237\n",
      "Epoch: 0, iteration: 190\n",
      "BBOX Regression Loss:  0.1369329269172998\n",
      "Classification Loss:  0.5179372697487677\n",
      "Epoch: 0, iteration: 200\n",
      "BBOX Regression Loss:  0.13012297792912084\n",
      "Classification Loss:  0.4922730126369882\n",
      "Epoch: 0, iteration: 210\n",
      "BBOX Regression Loss:  0.1240020760609333\n",
      "Classification Loss:  0.4690211268302835\n",
      "Epoch: 0, iteration: 220\n",
      "BBOX Regression Loss:  0.1183983546065524\n",
      "Classification Loss:  0.44786045078928227\n",
      "Epoch: 0, iteration: 230\n",
      "BBOX Regression Loss:  0.11326979698242856\n",
      "Classification Loss:  0.4285210529159329\n",
      "Epoch: 0, iteration: 240\n",
      "BBOX Regression Loss:  0.10861527222218738\n",
      "Classification Loss:  0.41077823236492683\n",
      "Epoch: 0, iteration: 250\n",
      "BBOX Regression Loss:  0.10437807714835637\n",
      "Classification Loss:  0.39444673527722013\n",
      "Epoch: 0, iteration: 260\n",
      "BBOX Regression Loss:  0.10039769490826556\n",
      "Classification Loss:  0.3793641400287336\n",
      "Epoch: 0, iteration: 270\n",
      "BBOX Regression Loss:  0.09669191921525856\n",
      "Classification Loss:  0.36539093317318355\n",
      "Epoch: 0, iteration: 280\n",
      "BBOX Regression Loss:  0.0932456566159436\n",
      "Classification Loss:  0.35240857720513036\n",
      "Epoch: 0, iteration: 290\n",
      "BBOX Regression Loss:  0.09014004125398979\n",
      "Classification Loss:  0.3403166968984432\n",
      "Epoch: 0, iteration: 300\n",
      "BBOX Regression Loss:  0.08718895051570862\n",
      "Classification Loss:  0.32902830060203503\n",
      "Epoch: 0, iteration: 310\n",
      "BBOX Regression Loss:  0.08439692207948686\n",
      "Classification Loss:  0.31846474271174185\n",
      "Epoch: 0, iteration: 320\n",
      "BBOX Regression Loss:  0.08176704992020623\n",
      "Classification Loss:  0.3085578207620764\n",
      "Epoch: 0, iteration: 330\n",
      "BBOX Regression Loss:  0.07929296226651741\n",
      "Classification Loss:  0.299247854367718\n",
      "Epoch: 0, iteration: 340\n",
      "BBOX Regression Loss:  0.07696306022280504\n",
      "Classification Loss:  0.29048241054058915\n",
      "Epoch: 0, iteration: 350\n",
      "BBOX Regression Loss:  0.07476570167715499\n",
      "Classification Loss:  0.2822151563533399\n",
      "Epoch: 0, iteration: 360\n",
      "BBOX Regression Loss:  0.0726900910386512\n",
      "Classification Loss:  0.2744048857001718\n",
      "Epoch: 0, iteration: 370\n",
      "BBOX Regression Loss:  0.07072650448998201\n",
      "Classification Loss:  0.2670147676253925\n",
      "Epoch: 0, iteration: 380\n",
      "BBOX Regression Loss:  0.06886860265251982\n",
      "Classification Loss:  0.2600118193559378\n",
      "Epoch: 0, iteration: 390\n",
      "BBOX Regression Loss:  0.06723517205455391\n",
      "Classification Loss:  0.2533681033327751\n",
      "Epoch: 0, iteration: 400\n",
      "BBOX Regression Loss:  0.06560201004672477\n",
      "Classification Loss:  0.2470579100296729\n",
      "Epoch: 0, iteration: 410\n",
      "BBOX Regression Loss:  0.0640178292367482\n",
      "Classification Loss:  0.2410546940499824\n",
      "Epoch: 0, iteration: 420\n",
      "BBOX Regression Loss:  0.062499128868920416\n",
      "Classification Loss:  0.23533628731851042\n",
      "Epoch: 0, iteration: 430\n",
      "BBOX Regression Loss:  0.061047804243036394\n",
      "Classification Loss:  0.22988248944673995\n",
      "Epoch: 0, iteration: 440\n",
      "BBOX Regression Loss:  0.05966147683738803\n",
      "Classification Loss:  0.22467537307657104\n",
      "Epoch: 0, iteration: 450\n",
      "BBOX Regression Loss:  0.0583362830217991\n",
      "Classification Loss:  0.21969860738963504\n",
      "Epoch: 0, iteration: 460\n",
      "BBOX Regression Loss:  0.05706854508535943\n",
      "Classification Loss:  0.21493725577825246\n",
      "Epoch: 0, iteration: 470\n",
      "BBOX Regression Loss:  0.05585468953002768\n",
      "Classification Loss:  0.2103776667791091\n",
      "Epoch: 0, iteration: 480\n",
      "BBOX Regression Loss:  0.05469136816499945\n",
      "Classification Loss:  0.2060073175214599\n",
      "Epoch: 0, iteration: 490\n",
      "BBOX Regression Loss:  0.05357550636599062\n",
      "Classification Loss:  0.20181468828748111\n",
      "Epoch: 0, iteration: 500\n",
      "BBOX Regression Loss:  0.05251774699148868\n",
      "Classification Loss:  0.19778921789331017\n",
      "Epoch: 0, iteration: 510\n",
      "BBOX Regression Loss:  0.051633268396077\n",
      "Classification Loss:  0.1939231518490312\n",
      "Epoch: 0, iteration: 520\n",
      "BBOX Regression Loss:  0.05067308938063449\n",
      "Classification Loss:  0.1902066816434535\n",
      "Epoch: 0, iteration: 530\n",
      "BBOX Regression Loss:  0.04972796010766947\n",
      "Classification Loss:  0.18663019301997175\n",
      "Epoch: 0, iteration: 540\n",
      "BBOX Regression Loss:  0.048811089398846964\n",
      "Classification Loss:  0.1831857263319455\n",
      "Epoch: 0, iteration: 550\n",
      "BBOX Regression Loss:  0.04792513875318385\n",
      "Classification Loss:  0.17986579756017287\n",
      "Epoch: 0, iteration: 560\n",
      "BBOX Regression Loss:  0.04707001969821697\n",
      "Classification Loss:  0.17666378128403926\n",
      "Epoch: 0, iteration: 570\n",
      "BBOX Regression Loss:  0.04624462624245306\n",
      "Classification Loss:  0.17357354076510229\n",
      "Epoch: 0, iteration: 580\n",
      "BBOX Regression Loss:  0.04544755482045678\n",
      "Classification Loss:  0.17058935158952626\n",
      "Epoch: 0, iteration: 590\n",
      "BBOX Regression Loss:  0.04467745871530378\n",
      "Classification Loss:  0.16770587527413539\n",
      "Epoch: 0, iteration: 600\n",
      "BBOX Regression Loss:  0.04393300718177479\n",
      "Classification Loss:  0.1649181206610031\n",
      "Epoch: 0, iteration: 610\n",
      "BBOX Regression Loss:  0.04321302708589871\n",
      "Classification Loss:  0.16222141313156802\n",
      "Epoch: 0, iteration: 620\n",
      "BBOX Regression Loss:  0.04262355035160829\n",
      "Classification Loss:  0.15961167334240672\n",
      "Epoch: 0, iteration: 630\n",
      "BBOX Regression Loss:  0.04201562640875255\n",
      "Classification Loss:  0.15708644386295498\n",
      "Epoch: 0, iteration: 640\n",
      "BBOX Regression Loss:  0.04137843408422365\n",
      "Classification Loss:  0.15464131582999402\n",
      "Epoch: 0, iteration: 650\n",
      "BBOX Regression Loss:  0.04074808115716373\n",
      "Classification Loss:  0.1522712002885647\n",
      "Epoch: 0, iteration: 660\n",
      "BBOX Regression Loss:  0.04013275893934697\n",
      "Classification Loss:  0.14997240177221913\n",
      "Epoch: 0, iteration: 670\n",
      "BBOX Regression Loss:  0.03953463662308401\n",
      "Classification Loss:  0.14774170069939327\n",
      "Epoch: 0, iteration: 680\n",
      "BBOX Regression Loss:  0.03895368247626077\n",
      "Classification Loss:  0.1455760980021383\n",
      "Epoch: 0, iteration: 690\n",
      "BBOX Regression Loss:  0.03838937426922879\n",
      "Classification Loss:  0.14347281789610095\n",
      "Epoch: 0, iteration: 700\n",
      "BBOX Regression Loss:  0.037841120729639896\n",
      "Classification Loss:  0.14142924046474037\n",
      "Epoch: 0, iteration: 710\n",
      "BBOX Regression Loss:  0.03730827876384651\n",
      "Classification Loss:  0.13944288633329463\n",
      "Epoch: 0, iteration: 720\n",
      "BBOX Regression Loss:  0.036790221656281825\n",
      "Classification Loss:  0.13751141085627644\n",
      "Epoch: 0, iteration: 730\n",
      "BBOX Regression Loss:  0.03628634697976524\n",
      "Classification Loss:  0.1356325885183629\n",
      "Epoch: 0, iteration: 740\n",
      "BBOX Regression Loss:  0.0357960833884048\n",
      "Classification Loss:  0.1338043120011941\n",
      "Epoch: 0, iteration: 750\n",
      "BBOX Regression Loss:  0.0353188881091328\n",
      "Classification Loss:  0.1320245788237655\n",
      "Epoch: 0, iteration: 760\n",
      "BBOX Regression Loss:  0.03485424566169755\n",
      "Classification Loss:  0.13029149091919637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, iteration: 770\n",
      "BBOX Regression Loss:  0.03440166661014418\n",
      "Classification Loss:  0.12860324595619876\n",
      "Epoch: 0, iteration: 780\n",
      "BBOX Regression Loss:  0.03396068730869219\n",
      "Classification Loss:  0.12695813066193856\n",
      "Epoch: 0, iteration: 790\n",
      "BBOX Regression Loss:  0.03353086787630961\n",
      "Classification Loss:  0.12535451996134506\n",
      "Epoch: 0, iteration: 800\n",
      "BBOX Regression Loss:  0.03311178984520767\n",
      "Classification Loss:  0.12379086605859574\n",
      "Epoch: 0, iteration: 810\n",
      "BBOX Regression Loss:  0.032703055722532265\n",
      "Classification Loss:  0.12226569913591716\n",
      "Epoch: 0, iteration: 820\n",
      "BBOX Regression Loss:  0.032304288861403435\n",
      "Classification Loss:  0.12077761830149138\n",
      "Epoch: 0, iteration: 830\n",
      "BBOX Regression Loss:  0.03191568758171548\n",
      "Classification Loss:  0.11932529177627242\n",
      "Epoch: 0, iteration: 840\n",
      "BBOX Regression Loss:  0.03157388883706835\n",
      "Classification Loss:  0.11790768060477197\n",
      "Epoch: 0, iteration: 850\n",
      "BBOX Regression Loss:  0.03121519331841741\n",
      "Classification Loss:  0.1165236320396711\n",
      "Epoch: 0, iteration: 860\n",
      "BBOX Regression Loss:  0.030855983893681466\n",
      "Classification Loss:  0.11517181149094298\n",
      "Epoch: 0, iteration: 870\n",
      "BBOX Regression Loss:  0.03050274557608893\n",
      "Classification Loss:  0.11385098895067552\n",
      "Epoch: 0, iteration: 880\n",
      "BBOX Regression Loss:  0.03015661598373198\n",
      "Classification Loss:  0.112560088412075\n",
      "Epoch: 0, iteration: 890\n",
      "BBOX Regression Loss:  0.029817967374313824\n",
      "Classification Loss:  0.11129808643717697\n",
      "Epoch: 0, iteration: 900\n",
      "BBOX Regression Loss:  0.02948674208840064\n",
      "Classification Loss:  0.11006402204506896\n",
      "Epoch: 0, iteration: 910\n",
      "BBOX Regression Loss:  0.029162757966082344\n",
      "Classification Loss:  0.1088569791875719\n",
      "Epoch: 0, iteration: 920\n",
      "BBOX Regression Loss:  0.02884580247677803\n",
      "Classification Loss:  0.1076760847549833\n",
      "Epoch: 0, iteration: 930\n",
      "BBOX Regression Loss:  0.02853565720491005\n",
      "Classification Loss:  0.10652050053756812\n",
      "Epoch: 0, iteration: 940\n",
      "BBOX Regression Loss:  0.028232108577413373\n",
      "Classification Loss:  0.10538942553614869\n",
      "Epoch: 0, iteration: 950\n",
      "BBOX Regression Loss:  0.0279349496446095\n",
      "Classification Loss:  0.10428209096955801\n",
      "Epoch: 0, iteration: 960\n",
      "BBOX Regression Loss:  0.02764406301384825\n",
      "Classification Loss:  0.10319776138643603\n",
      "Epoch: 0, iteration: 970\n",
      "BBOX Regression Loss:  0.027394026369246115\n",
      "Classification Loss:  0.10213583884241557\n",
      "Epoch: 0, iteration: 980\n",
      "BBOX Regression Loss:  0.027133186502248358\n",
      "Classification Loss:  0.10109585210515887\n",
      "Epoch: 0, iteration: 990\n",
      "BBOX Regression Loss:  0.026865951092117906\n",
      "Classification Loss:  0.10007700528081936\n",
      "Epoch: 0, iteration: 1000\n",
      "BBOX Regression Loss:  0.026599270464212035\n",
      "Classification Loss:  0.09907850338342704\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weights/Ｗeights.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c99d322e3af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classification Loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;34m'''儲存權重'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mssd_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weights/Ｗeights.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights/Ｗeights.pth'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    n=0\n",
    "    loss_sum=[]\n",
    "    loc_loss=[]\n",
    "    conf_loss=[]\n",
    "    for number__ in range(iteration) :\n",
    "        '''要用Variable包裝tensor才能送入模型'''\n",
    "        if Use_cuda:\n",
    "            image_ = Variable(image_in.cuda())\n",
    "            y = [Variable(torch.tensor(label_0).cuda(), volatile=True),Variable(torch.tensor(label_1).cuda(), \n",
    "                volatile=True),Variable(torch.tensor(label_2).cuda(), volatile=True),Variable(torch.tensor(label_3).cuda(), volatile=True)]      \n",
    "        else:\n",
    "            image_ = Variable(image_in)\n",
    "            y = [Variable(torch.tensor(label_0), volatile=True),Variable(torch.tensor(label_1), \n",
    "                volatile=True),Variable(torch.tensor(label_2), volatile=True),Variable(torch.tensor(label_3), volatile=True)]\n",
    "\n",
    "        '''Forward Pass'''\n",
    "        out = net(image_)\n",
    "        '''Regression Loss and Classification Loss'''\n",
    "        loss_l,loss_c = criterion(out,y )\n",
    "        loss = loss_l+ loss_c\n",
    "        '''Backward'''\n",
    "        loss.backward()\n",
    "\n",
    "        loc_loss.append(loss_l.data.cpu().numpy())\n",
    "        conf_loss.append(loss_c.data.cpu().numpy())\n",
    "        loss_sum.append(loss.data.cpu().numpy())\n",
    "        '''更新參數'''\n",
    "        optimizer.step()\n",
    "        '''清空Gradients'''\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        n+=1\n",
    "        if n%10==0:\n",
    "            print('Epoch: %d, iteration: %d'% (epoch,  n))\n",
    "            print('BBOX Regression Loss: ', np.mean(loc_loss))\n",
    "            print('Classification Loss: ', np.mean(conf_loss))\n",
    "    '''儲存權重'''\n",
    "    torch.save(ssd_net.state_dict(),'weights/Ｗeights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
